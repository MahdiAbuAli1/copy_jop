# 1_knowledge_pipeline_ocr/loaders_ocr.py

import os
import json
from typing import List, Tuple, Optional
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
import numpy as np
import cv2  # OpenCV for image pre-processing

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    UnstructuredWordDocumentLoader,
    TextLoader,
)

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª OCR ---
TESSERACT_LANG = 'ara+eng'
# PSM 3: ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„ØµÙØ­Ø©ØŒ ÙˆÙ‡Ùˆ Ø®ÙŠØ§Ø± Ù‚ÙˆÙŠ ÙˆÙ…ØªÙˆØ§Ø²Ù†.
TESSERACT_CONFIG = '--psm 3 --dpi 300'

def get_best_ocr_result(image_bytes: bytes) -> str:
    """
    ÙŠØ·Ø¨Ù‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±Ø© ÙˆÙŠØ®ØªØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© OCR.
    """
    nparr = np.frombuffer(image_bytes, np.uint8)
    original_img_cv = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    if original_img_cv is None:
        return ""

    gray_img = cv2.cvtColor(original_img_cv, cv2.COLOR_BGR2GRAY)

    # --- Ù‚Ø§Ø¦Ù…Ø© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ---
    processing_strategies = {
        "standard": preprocess_standard,
        "inverted": preprocess_inverted,
        "upscaled_denoised": preprocess_upscaled_denoised,
    }

    results = {}

    for name, strategy_func in processing_strategies.items():
        try:
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processed_img = strategy_func(gray_img.copy())
            pil_image = Image.fromarray(processed_img)
            
            # ØªØ´ØºÙŠÙ„ OCR
            text = pytesseract.image_to_string(
                pil_image, 
                lang=TESSERACT_LANG,
                config=TESSERACT_CONFIG
            ).strip()
            
            # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªÙŠØ¬Ø©
            if is_text_meaningful(text):
                results[name] = text
                print(f"        - (Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: {name}) -> ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù†Øµ: '{text[:40].replace(chr(10), ' ')}...'")
        except Exception as e:
            print(f"        - âš ï¸ ÙØ´Ù„Øª Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© '{name}': {e}")
            continue

    # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù†ØªØ§Ø¦Ø¬ Ø¬ÙŠØ¯Ø©ØŒ Ø£Ø±Ø¬Ø¹ Ø³Ù„Ø³Ù„Ø© ÙØ§Ø±ØºØ©
    if not results:
        return ""

    # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ
    best_strategy = max(results, key=lambda k: len(results[k]))
    print(f"        - âœ¨ ØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©: '{best_strategy}'")
    return results[best_strategy]

# --- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ÙƒÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© ---

def preprocess_standard(image: np.ndarray) -> np.ndarray:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‚ÙŠØ§Ø³ÙŠØ©: ØªØ¨Ø§ÙŠÙ† ÙÙ‚Ø·."""
    return cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

def preprocess_inverted(image: np.ndarray) -> np.ndarray:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹ÙƒØ³ÙŠØ©: Ø¹ÙƒØ³ Ø§Ù„Ø£Ù„ÙˆØ§Ù† Ø«Ù… Ø§Ù„ØªØ¨Ø§ÙŠÙ†."""
    inverted = cv2.bitwise_not(image)
    return cv2.adaptiveThreshold(inverted, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)

def preprocess_upscaled_denoised(image: np.ndarray) -> np.ndarray:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø©: ØªÙƒØ¨ÙŠØ±ØŒ Ø¥Ø²Ø§Ù„Ø© ØªØ´ÙˆÙŠØ´ØŒ Ø«Ù… ØªØ¨Ø§ÙŠÙ†."""
    scale_factor = 2.0
    width = int(image.shape[1] * scale_factor)
    height = int(image.shape[0] * scale_factor)
    resized = cv2.resize(image, (width, height), interpolation=cv2.INTER_LANCZOS4)
    denoised = cv2.fastNlMeansDenoising(resized, h=30, templateWindowSize=7, searchWindowSize=21)
    return cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 5)

def is_text_meaningful(text: str, min_chars: int = 10, min_words: int = 2) -> bool:
    """
    Ø¯Ø§Ù„Ø© Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø°Ø§ Ù…Ø¹Ù†Ù‰.
    """
    text = text.strip()
    if len(text) < min_chars:
        return False
    words = text.split()
    if len(words) < min_words:
        return False
    alpha_chars = sum(1 for char in text if char.isalpha())
    if len(text) > 0 and alpha_chars / len(text) < 0.5:
        return False
    return True

# --- Ø§Ù„Ù…Ø­Ù…Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ---
class MultiStrategyOcrPdfLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path
        print(f"ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù„Ù„Ù…Ù„Ù: {os.path.basename(self.file_path)}")

    def load(self) -> List[Document]:
        docs = []
        try:
            pdf_document = fitz.open(self.file_path)
            print(f"    - ğŸ“– Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© {len(pdf_document)} ØµÙØ­Ø©...")
            
            for page_num, page in enumerate(pdf_document):
                normal_text = page.get_text("text").strip()
                ocr_texts = []
                image_list = page.get_images(full=True)
                
                if image_list:
                    print(f"      - ğŸ–¼ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_list)} ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num + 1}. ØªØ·Ø¨ÙŠÙ‚ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ù…ØªØ¹Ø¯Ø¯Ø©...")
                    for img_index, img in enumerate(image_list):
                        xref = img[0]
                        base_image = pdf_document.extract_image(xref)
                        image_bytes = base_image["image"]
                        
                        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© OCR Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¬Ø±Ø¨Ø© Ø¹Ø¯Ø© Ø·Ø±Ù‚
                        best_text = get_best_ocr_result(image_bytes)
                        if best_text:
                            ocr_texts.append(best_text)

                page_content_parts = []
                if normal_text:
                    page_content_parts.append("--- Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ ---\n" + normal_text)
                if ocr_texts:
                    full_ocr_text = "\n\n".join(ocr_texts)
                    page_content_parts.append("--- Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ØµÙˆØ± (OCR) ---\n" + full_ocr_text)
                
                final_page_content = "\n\n".join(page_content_parts)
                
                if final_page_content:
                    metadata = {"source": self.file_path, "page": page_num + 1}
                    docs.append(Document(page_content=final_page_content, metadata=metadata))
                
            pdf_document.close()
        except Exception as e:
            print(f"    - âŒ ÙØ´Ù„ ÙƒØ¨ÙŠØ± ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF '{self.file_path}'. Ø§Ù„Ø®Ø·Ø£: {e}")
            return []
            
        return docs

# --- ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØ­Ù…ÙŠÙ„ ---
LOADER_MAPPING = {
    ".pdf": MultiStrategyOcrPdfLoader,  # <-- âœ¨âœ¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù‡Ù†Ø§ âœ¨âœ¨
    ".docx": UnstructuredWordDocumentLoader,
    ".txt": TextLoader,
}

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ---
def load_documents(source_dir: str) -> Tuple[List[Document], Optional[str]]:
    # ... (Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§ Ù„Ù… ÙŠØªØºÙŠØ±) ...
    all_documents = []
    entity_name = None
    config_file_path = os.path.join(source_dir, "config.json")

    print(f"ğŸ“‚ Ø¬Ø§Ø±Ù Ø§Ù„Ù…Ø³Ø­ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: '{source_dir}'")

    if not os.path.isdir(source_dir):
        raise ValueError(f"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„ÙŠØ³ Ù…Ø¬Ù„Ø¯Ù‹Ø§ ØµØ§Ù„Ø­Ù‹Ø§: {source_dir}")

    if os.path.exists(config_file_path):
        try:
            with open(config_file_path, "r", encoding="utf-8") as f:
                config_data = json.load(f)
                entity_name = config_data.get("entity_name")
                if entity_name:
                    print(f"  - âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ÙƒÙŠØ§Ù†: '{entity_name}'")
        except Exception as e:
            print(f"  - âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© 'config.json': {e}")
    else:
        print(f"  - âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù 'config.json'.")

    for filename in os.listdir(source_dir):
        if filename == "config.json" or filename.startswith('.'):
            continue
        
        file_path = os.path.join(source_dir, filename)
        if not os.path.isfile(file_path):
            continue

        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext in LOADER_MAPPING:
            loader_class = LOADER_MAPPING[file_ext]
            print(f"  - ğŸ“„ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: '{filename}'...")
            try:
                loader = loader_class(file_path)
                loaded_docs = loader.load()
                all_documents.extend(loaded_docs)
                print(f"    - âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© {len(loaded_docs)} ØµÙØ­Ø©.")
            except Exception as e:
                print(f"    - âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù '{filename}'. Ø§Ù„Ø®Ø·Ø£: {e}")
        else:
            print(f"  -  ØªÙ… ØªØ®Ø·ÙŠ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: '{filename}'")

    if not all_documents:
        print(" Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
    
    print(f"\n Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„. Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(all_documents)}")
    return all_documents, entity_name
