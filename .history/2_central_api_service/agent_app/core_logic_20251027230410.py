# # src/app/core_logic.py
# #ÙƒÙˆØ¯ Ù…Ù…ØªØ§Ø² Ø§Ø«Ø¨Øª Ø¬Ø¯Ø§Ø±ØªÙ‡ ÙˆÙ†ØªØ§Ø¦Ø¬ Ù…Ù…ØªØ§Ø²Ù‡
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain.chains import create_history_aware_retriever, create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.messages import HumanMessage, AIMessage

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# # --- Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø³ÙƒØ±Øª Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ---
# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© Ø§Ù„ØªÙŠ ÙŠØ¨Ù†ÙŠÙ‡Ø§ Ø³ÙƒØ±Øª main_builder.py ---
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# embeddings: OllamaEmbeddings = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {} 
# initialization_lock = asyncio.Lock()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
# async def initialize_agent():
#     global llm, embeddings, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Øª 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© get_answer_stream ---
# async def get_answer_stream(request_info: dict) -> AsyncGenerator[Dict, None]:
#     question = request_info["question"].strip()
#     tenant_id = request_info.get("tenant_id")
#     session_id = tenant_id or "default_session"

#     if not vector_store:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     # --- Ø§Ù„ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØªÙ… Ù‡Ù†Ø§ØŒ ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¨Ø­Ø« ---
#     retriever = vector_store.as_retriever(
#         search_kwargs={'k': 15, 'filter': {'tenant_id': tenant_id}}
#     )
    
#     user_chat_history = chat_history.get(session_id, [])

#     history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
# src/app/core_logic.py
# src/app/core_logic.py

import os
import logging
import asyncio
import httpx
import yaml
from typing import AsyncGenerator, Dict, List

from dotenv import load_dotenv
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.messages import HumanMessage, AIMessage

from rank_bm25 import BM25Okapi
from rapidfuzz import fuzz
import re

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")

UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")
FAQ_PATH = os.path.join(PROJECT_ROOT, "kb", "faqs.yaml")

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
llm: Ollama = None
vector_store: FAISS = None
embeddings: OllamaEmbeddings = None
bm25: BM25Okapi = None
faq_items: List[Dict] = []
chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
initialization_lock = asyncio.Lock()

# --- 2. Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ---
TASHKEEL = re.compile(r'[\u0617-\u061A\u064B-\u0652]')

def normalize_ar(text: str) -> str:
    t = text.strip()
    t = TASHKEEL.sub("", t)
    t = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", t)
    t = re.sub("Ù‰", "ÙŠ", t)
    t = re.sub("Ø¤", "Ùˆ", t)
    t = re.sub("Ø¦", "ÙŠ", t)
    t = re.sub("Ø©", "Ù‡", t)
    return t

# --- 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ prompts ---
REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

ANSWER_PROMPT = ChatPromptTemplate.from_template("""
Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ¯ÙˆØ¯ ÙˆØ®Ø¨ÙŠØ±.  
- Ø±Ø­Ø¨ Ø¨Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±.  
- Ø­Ø§ÙˆÙ„ ÙÙ‡Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ù‡Ù…Ø§ ÙƒØ§Ù†Øª Ù…Ø¹Ù‚Ø¯Ø© Ø£Ùˆ Ù…Ø®ØªØµØ±Ø©.  
- Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·ØŒ ÙˆØ¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¬ÙˆØ§Ø¨ Ù‚Ù„: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø©."  
- Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† Ø§Ù„Ø¥Ù†Ø³Ø§Ù†.  

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# --- 4. Ø£Ø³Ø¦Ù„Ø© ØªÙØ§Ø¹Ù„ÙŠØ© Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© ---
SOCIAL_QS = {
    "Ø³Ù„Ø§Ù…": ["Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ù‡Ù„Ø§", "Ù…Ø±Ø­Ø¨Ø§", "Ø§Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù…"],
    "Ø´ÙƒØ±": ["Ø´ÙƒØ±Ø§", "Ù…Ø´ÙƒÙˆØ±", "Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±", "Ù…ØªØ´ÙƒØ±"]
}

SOCIAL_RESPONSES = {
    "Ø³Ù„Ø§Ù…": "ÙˆØ¹Ù„ÙŠÙƒÙ… Ø§Ù„Ø³Ù„Ø§Ù…! ÙƒÙŠÙ Ø£Ø³ØªØ·ÙŠØ¹ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
    "Ø´ÙƒØ±": "Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø­Ø¨ ÙˆØ§Ù„Ø³Ø¹Ø©! ğŸ˜Š"
}

def check_social(question: str):
    for key, variants in SOCIAL_QS.items():
        for v in variants:
            if fuzz.partial_ratio(question, v) > 80:
                return SOCIAL_RESPONSES[key]
    return None

# --- 5. Ø¯Ø§Ù„Ø© FAQ ---
def faq_lookup(question: str, tenant_id: str, threshold: int = 80):
    if not bm25 or not faq_items:
        return False, {}
    # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ tenant_id
    relevant_faqs = [f for f in faq_items if f.get("tenant_id") == tenant_id]
    if not relevant_faqs:
        return False, {}
    q_norm = normalize_ar(question)
    corpus_tokens = [normalize_ar(item["title"] + " " + " ".join(item.get("question_variants", []))).split()
                     for item in relevant_faqs]
    local_bm25 = BM25Okapi(corpus_tokens)
    scores = local_bm25.get_scores(q_norm.split())
    top_idx = scores.argmax()
    if scores[top_idx] >= threshold:
        return True, relevant_faqs[top_idx]
    # fallback: fuzzy match
    for item in relevant_faqs:
        text = item["title"] + " " + " ".join(item.get("question_variants", []))
        if fuzz.token_set_ratio(question, text) > threshold:
            return True, item
    return False, {}

# --- 6. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ ---
async def initialize_agent():
    global llm, embeddings, vector_store, bm25, faq_items
    async with initialization_lock:
        if vector_store is not None:
            return
        logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆFAQ ÙˆFAISS...")
        try:
            # --- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§ØªØµØ§Ù„ ---
            async with httpx.AsyncClient() as client:
                await client.get(OLLAMA_HOST, timeout=10.0)

            # --- ØªÙ‡ÙŠØ¦Ø© LLM ÙˆEmbeddings ---
            llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)

            # --- ØªØ­Ù…ÙŠÙ„ FAISS ---
            if not os.path.isdir(UNIFIED_DB_PATH):
                raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Øª 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")

            vector_store = await asyncio.to_thread(
                FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
            )

            # --- ØªØ­Ù…ÙŠÙ„ FAQ ---
            with open(FAQ_PATH, "r", encoding="utf-8") as f:
                kb_yaml = yaml.safe_load(f)
                faq_items = kb_yaml.get("faqs", [])

            # --- Ø¥Ø¹Ø¯Ø§Ø¯ BM25 Ø¹Ø§Ù… Ù„Ù„Ù€ FAQ ---
            corpus_tokens = [normalize_ar(item["title"] + " " + " ".join(item.get("question_variants", []))).split()
                             for item in faq_items]
            bm25 = BM25Okapi(corpus_tokens)

            logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„: FAISS + FAQ + BM25 + ØªÙØ§Ø¹Ù„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ.")
        except Exception as e:
            logging.error(f"ÙØ´Ù„ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            raise

# --- 7. Ø¯Ø§Ù„Ø© get_answer_stream Ù…Ø­Ø³Ù‘Ù†Ø© ---
async def get_answer_stream(request_info: dict) -> AsyncGenerator[Dict, None]:
    question = request_info["question"].strip()
    tenant_id = request_info.get("tenant_id")
    session_id = tenant_id or "default_session"

    if not vector_store:
        yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
        return

    user_chat_history = chat_history.get(session_id, [])

    # --- 1) ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ­ÙŠØ© Ø£Ùˆ Ø§Ù„Ø´ÙƒØ± Ø£ÙˆÙ„Ø§Ù‹ ---
    social_resp = check_social(question)
    if social_resp:
        user_chat_history.append(HumanMessage(content=question))
        user_chat_history.append(AIMessage(content=social_resp))
        chat_history[session_id] = user_chat_history[-10:]
        yield {"type": "social", "content": social_resp}
        return

    # --- 2) ØªØ­Ù‚Ù‚ FAQ ---
    isfaq, faq_item = faq_lookup(question, tenant_id)
    if isfaq:
        answer_text = faq_item.get("answer", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¬ÙˆØ§Ø¨ Ù…Ø­Ø¯Ø¯ ÙÙŠ FAQ.")
        user_chat_history.append(HumanMessage(content=question))
        user_chat_history.append(AIMessage(content=answer_text))
        chat_history[session_id] = user_chat_history[-10:]
        yield {"type": "faq", "content": answer_text}
        return

    # --- 3) retrieval Ù‡Ø¬ÙŠÙ†ÙŠ: FAISS + LLM ---
    retriever = vector_store.as_retriever(
        search_kwargs={'k': 5, 'filter': {'tenant_id': tenant_id}}  # ØªÙ‚Ù„ÙŠÙ„ k Ù„ØªØ³Ø±ÙŠØ¹
    )
    history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
    document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
    conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

    logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}' Ø¹Ø¨Ø± pipeline Ù‡Ø¬ÙŠÙ†ÙŠ...")
    try:
        full_answer = ""
        async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
            if "answer" in chunk and chunk["answer"] is not None:
                answer_chunk = chunk["answer"]
                full_answer += answer_chunk
                yield {"type": "chunk", "content": answer_chunk}

        # ØªØ­Ø¯ÙŠØ« Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        user_chat_history.append(HumanMessage(content=question))
        user_chat_history.append(AIMessage(content=full_answer))
        chat_history[session_id] = user_chat_history[-10:]

        logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
    except Exception as e:
        logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
        yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}

