# 2_evaluation_suite/run_evaluation.py

import os
import json
import argparse
import time
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„ØªÙŠ Ø¨Ù†ÙŠÙ†Ø§Ù‡Ø§
from core.retriever_factory import get_retriever, RetrieverType
from core.evaluators import evaluate_retrieval

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© ---
load_dotenv()
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
TEST_CASES_DIR = os.path.join(BASE_DIR, "test_cases")
RESULTS_DIR = os.path.join(BASE_DIR, "results")
ALL_DOCS_CACHE = {} # Ø°Ø§ÙƒØ±Ø© ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„ØªØ¬Ù†Ø¨ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø§Ø±Ø§Ù‹

def load_all_documents_from_kb() -> List[Dict[str, Any]]:
    """
    ÙŠØ­Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©.
    Ù‡Ø°Ø§ Ø¶Ø±ÙˆØ±ÙŠ Ù„ØªÙ‡ÙŠØ¦Ø© BM25 Retriever.
    """
    # Ù‡Ø°Ø§ Ø¬Ø²Ø¡ Ù…ØªÙ‚Ø¯Ù… Ù‚Ù„ÙŠÙ„Ø§Ù‹ØŒ Ù„ÙƒÙ†Ù‡ Ø¶Ø±ÙˆØ±ÙŠ. Ù†Ø­Ù† Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù€ BM25.
    # Ø³Ù†Ù‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© FAISS ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù†Ù‡Ø§.
    from langchain_community.vectorstores import FAISS
    from langchain_community.embeddings import OllamaEmbeddings
    
    db_path = os.path.abspath(os.path.join(BASE_DIR, "../3_shared_resources/vector_db/"))
    if not os.path.exists(os.path.join(db_path, "index.faiss")):
        raise FileNotFoundError("Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©!")
        
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
    db = FAISS.load_local(db_path, embeddings, allow_dangerous_deserialization=True)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³
    # db.docstore._dict Ù‡ÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø© ÙÙŠ FAISS
    all_docs = list(db.docstore._dict.values())
    return all_docs


def run_test_for_tenant(
    tenant_id: str,
    retriever_type: RetrieverType,
    all_docs: List[Any]
) -> List[Dict[str, Any]]:
    """
    ÙŠØ´ØºÙ„ Ø¬Ù…ÙŠØ¹ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ø­Ø¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙˆØ¹ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ø­Ø¯Ø¯.
    """
    print("\n" + "="*30 + f" ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„: {tenant_id} | Ø§Ù„Ù†ÙˆØ¹: {retriever_type} " + "="*30)
    
    # --- 1. ØªØ­Ù…ÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
    test_cases_file = os.path.join(TEST_CASES_DIR, f"{tenant_id}_cases.json")
    if not os.path.exists(test_cases_file):
        print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        return []
    
    with open(test_cases_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)
    print(f"  - ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(test_cases)} Ø­Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±.")

    # --- 2. ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ ---
    # Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØµÙÙŠØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù„ØªØ´Ù…Ù„ ÙÙ‚Ø· ØªÙ„Ùƒ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù€ BM25
    tenant_specific_docs = [doc for doc in all_docs if doc.metadata.get("tenant_id") == tenant_id]
    if not tenant_specific_docs:
        print(f"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. ØªÙ… Ø§Ù„ØªØ®Ø·ÙŠ.")
        return []

    retriever = get_retriever(retriever_type, tenant_specific_docs, EMBEDDING_MODEL_NAME, k=5)

    # --- 3. ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ---
    results = []
    for case in test_cases:
        question = case["question"]
        print(f"\n--- â“ Ø§Ø®ØªØ¨Ø§Ø± [{case['case_id']}]: {question} ---")
        
        start_time = time.time()
        retrieved_docs_langchain = retriever.invoke(question)
        end_time = time.time()
        
        # ØªØ­ÙˆÙŠÙ„ ÙƒØ§Ø¦Ù†Ø§Øª Langchain Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ù…ÙŠØ³ Ø¨Ø³ÙŠØ·Ø© Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØ§Ù„Ø­ÙØ¸
        retrieved_docs_simple = [
            {"content": doc.page_content, "source": doc.metadata.get("source", "N/A")}
            for doc in retrieved_docs_langchain
        ]
        
        # --- 4. ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ---
        evaluation = evaluate_retrieval(
            retrieved_docs=retrieved_docs_simple,
            expected_keywords=case["expected_keywords"],
            expected_source=case["expected_source"]
        )
        
        print(f"  - ğŸ“Š Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {evaluation['status']} (Ø§Ù„Ù…ØµØ¯Ø±: {evaluation['source_check']}, Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {evaluation['keyword_evaluation']['score']})")
        
        results.append({
            "case_id": case["case_id"],
            "question": question,
            "retrieval_time_seconds": round(end_time - start_time, 2),
            "evaluation": evaluation,
            "retrieved_documents": retrieved_docs_simple
        })
        
    return results


def save_results(tenant_id: str, retriever_type: RetrieverType, results: List[Dict[str, Any]]):
    """
    ÙŠØ­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠ Ù…Ù„Ù JSON Ù…Ù†Ø¸Ù….
    """
    if not results:
        return
        
    os.makedirs(RESULTS_DIR, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_{tenant_id}_{retriever_type}.json"
    filepath = os.path.join(RESULTS_DIR, filename)
    
    report = {
        "report_info": {
            "tenant_id": tenant_id,
            "retriever_type": retriever_type,
            "timestamp": datetime.now().isoformat(),
            "embedding_model": EMBEDDING_MODEL_NAME,
            "total_cases": len(results)
        },
        "evaluation_results": results
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=4)
        
    print("\n" + f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙØµÙ„ ÙÙŠ: {filepath}")


def main():
    parser = argparse.ArgumentParser(description="Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹.")
    parser.add_argument(
        "--tenant", 
        type=str, 
        required=True, 
        help="Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø®ØªØ¨Ø§Ø±Ù‡ (Ù…Ø«Ø§Ù„: accredit, scholl). Ø§ÙƒØªØ¨ 'all' Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡."
    )
    parser.add_argument(
        "--retriever", 
        type=str, 
        default="all", 
        choices=["all", "ensemble", "faiss", "bm25"],
        help="Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ø®ØªØ¨Ø§Ø±Ù‡."
    )
    args = parser.parse_args()

    # --- ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ---
    print("[*] Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† (Ù…Ø·Ù„ÙˆØ¨ Ù„Ù€ BM25)...")
    try:
        all_docs_from_kb = load_all_documents_from_kb()
        print(f"[âœ…] ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(all_docs_from_kb)} Ù‚Ø·Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­.")
    except Exception as e:
        print(f"[âŒ] ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©. Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©. Ø§Ù„Ø®Ø·Ø£: {e}")
        return

    # --- ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ ÙˆØ§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø§Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
    tenants_to_test = [d for d in os.listdir(TEST_CASES_DIR) if d.endswith('_cases.json')]
    tenants_to_test = [d.replace('_cases.json', '') for d in tenants_to_test]
    
    if args.tenant != "all":
        if args.tenant not in tenants_to_test:
            print(f"Ø®Ø·Ø£: Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ù„Ù Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù„Ù„Ø¹Ù…ÙŠÙ„ '{args.tenant}'.")
            return
        tenants_to_test = [args.tenant]

    retrievers_to_test = ["ensemble", "faiss", "bm25"] if args.retriever == "all" else [args.retriever]

    # --- Ø¨Ø¯Ø¡ Ø­Ù„Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---
    for tenant in tenants_to_test:
        for retriever_name in retrievers_to_test:
            test_results = run_test_for_tenant(tenant, retriever_name, all_docs_from_kb)
            save_results(tenant, retriever_name, test_results)
            
    print("\n" + "="*70 + "\nğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰\n" + "="*70)
    print(f"ğŸ” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯: {RESULTS_DIR}")


if __name__ == "__main__":
    main()
