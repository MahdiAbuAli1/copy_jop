# 1_knowledge_pipeline/splitters.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø©)
# -----------------------------------------------------------------------------
# Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ­Ø¯Ø© Ù…Ø³Ø¤ÙˆÙ„Ø© Ø¹Ù† ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ© Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£ØµØºØ± (chunks).
# -----------------------------------------------------------------------------

from typing import List
from langchain_core.documents import Document
# ğŸ”´ğŸ”´ğŸ”´ --- ØªÙ… ØªØ­Ø¯ÙŠØ« Ø³Ø·Ø± Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù‡Ø°Ø§ --- ğŸ”´ğŸ”´ğŸ”´
# Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø·Ø± Ø§Ù„ØµØ­ÙŠØ­
from langchain_text_splitters import RecursiveCharacterTextSplitter


# --- ØªØ¹Ø±ÙŠÙ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªÙ‚Ø·ÙŠØ¹ ---
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 250

def split_documents(documents: List[Document]) -> List[Document]:
    """
    ØªÙ‚ÙˆÙ… Ø¨ØªÙ‚Ø³ÙŠÙ… Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£ØµØºØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RecursiveCharacterTextSplitter.

    Args:
        documents (List[Document]): Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©.

    Returns:
        List[Document]: Ù‚Ø§Ø¦Ù…Ø© Ø¬Ø¯ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù‚Ø·Ø¹ (Chunks).
    """
    print(f"\n[+] Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙ‚Ø·ÙŠØ¹ {len(documents)} Ø¬Ø²Ø¡/ØµÙØ­Ø© Ø¥Ù„Ù‰ Ù‚Ø·Ø¹ Ø£ØµØºØ±...")
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        separators=["\n\n", "\n", ". ", "ØŒ ", " ", ""]
    )

    chunks = text_splitter.split_documents(documents)

    print(f"[*] Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ‚Ø·ÙŠØ¹. Ù†ØªØ¬ Ø¹Ù†Ù‡ {len(chunks)} Ù‚Ø·Ø¹Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ù‡Ø§Ø¦ÙŠØ©.")
    return chunks
