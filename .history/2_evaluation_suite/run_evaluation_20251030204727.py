# 2_evaluation_suite/run_evaluation.py (Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…ØµØ­Ø­Ø©)

import os
import argparse
import json
import time
from datetime import datetime
from dotenv import load_dotenv
from typing import List, Dict, Any

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
from core.retriever_factory import get_retriever
from core.evaluators import evaluate_retrieval
from core.reranker import rerank_documents, cross_encoder

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© ---
load_dotenv()
EMBEDDING_MODEL_NAME = os.getenv("EMBEDDING_MODEL_NAME")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTOR_DB_BASE_DIR = os.path.abspath(os.path.join(BASE_DIR, "../3_shared_resources/vector_db/"))
TEST_CASES_DIR = os.path.join(BASE_DIR, "test_cases")
RESULTS_DIR = os.path.join(BASE_DIR, "results")
os.makedirs(RESULTS_DIR, exist_ok=True)

def load_all_docs_from_faiss() -> List[Document]:
    """
    ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ø·Ø¹ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø²ÙˆÙ„Ø©.
    Ù‡Ø°Ø§ Ù…Ø·Ù„ÙˆØ¨ Ù„ØªÙ‡ÙŠØ¦Ø© BM25Retriever.
    """
    print("[*] Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†...")
    all_docs = []
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ù‡ÙŠØ£
    try:
        embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
    except Exception as e:
        print(f"[âŒ] ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {e}")
        return []

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    if not os.path.exists(VECTOR_DB_BASE_DIR) or not os.listdir(VECTOR_DB_BASE_DIR):
        print("[âŒ] ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: Ø§Ù„Ù…Ø¬Ù„Ø¯ 'vector_db' ÙØ§Ø±Øº Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return []

    # Ø§Ù„Ù…Ø±ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„ Ù…Ø¬Ù„Ø¯ Ø¹Ù…ÙŠÙ„ ÙˆØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§ØªÙ‡
    for tenant_id in os.listdir(VECTOR_DB_BASE_DIR):
        tenant_db_path = os.path.join(VECTOR_DB_BASE_DIR, tenant_id)
        if os.path.isdir(tenant_db_path) and os.path.exists(os.path.join(tenant_db_path, "index.faiss")):
            try:
                vector_store = FAISS.load_local(tenant_db_path, embeddings, allow_dangerous_deserialization=True)
                # Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                # Ù†Ø³ØªØ®Ø¯Ù… retriever Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§ØªÙ‡Ø§ Ø§Ù„ÙˆØµÙÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
                retriever = vector_store.as_retriever(search_type="mmr", search_kwargs={'k': 1000})
                docs = retriever.get_relevant_documents(query=" ") # Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙØ§Ø±Øº Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø¹ÙŠÙ†Ø© ÙƒØ¨ÙŠØ±Ø©
                all_docs.extend(docs)
            except Exception as e:
                print(f"  - âš ï¸ ØªØ­Ø°ÙŠØ±: ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}'. Ø§Ù„Ø®Ø·Ø£: {e}")
    
    if not all_docs:
        print("[âŒ] ÙØ´Ù„ Ø­Ø§Ø³Ù… ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯Ø§Øª ÙÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ±Ø¹ÙŠØ©!")
        return []

    print(f"[âœ…] ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(all_docs)} Ù‚Ø·Ø¹Ø© Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù† Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
    return all_docs


def run_test_for_tenant(tenant_id: str, retriever_types: List[str], all_docs: List[Document]):
    """ÙŠÙ†ÙØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ø¹Ù…ÙŠÙ„ ÙˆØ§Ø­Ø¯."""
    test_case_file = os.path.join(TEST_CASES_DIR, f"{tenant_id}_cases.json")
    if not os.path.exists(test_case_file):
        print(f"  - âš ï¸ ØªÙ… ØªØ®Ø·ÙŠ Ø§Ù„Ø¹Ù…ÙŠÙ„ '{tenant_id}': Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±.")
        return

    with open(test_case_file, 'r', encoding='utf-8') as f:
        test_cases = json.load(f)

    for retriever_type in retriever_types:
        print("\n" + "="*30 + f" ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¹Ù…ÙŠÙ„: {tenant_id} | Ø§Ù„Ù†ÙˆØ¹: {retriever_type} " + "="*30)
        print(f"  - ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(test_cases)} Ø­Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±.")

        report = {
            "report_info": {
                "tenant_id": tenant_id,
                "retriever_type": retriever_type,
                "timestamp": datetime.now().isoformat(),
                "embedding_model": EMBEDDING_MODEL_NAME,
                "total_cases": len(test_cases)
            },
            "evaluation_results": []
        }
        
        try:
            retriever = get_retriever(retriever_type, OllamaEmbeddings(model=EMBEDDING_MODEL_NAME), tenant_id, all_docs)
        except (FileNotFoundError, ValueError, RuntimeError) as e:
            print(f"  - âŒ ÙØ´Ù„ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹. ØªØ®Ø·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±. Ø§Ù„Ø®Ø·Ø£: {e}")
            continue

        for case in test_cases:
            print(f"\n--- â“ Ø§Ø®ØªØ¨Ø§Ø± [{case['case_id']}]: {case['question']} ---")
            
            start_time = time.time()
            retrieved_docs_langchain = retriever.get_relevant_documents(case['question'])
            retrieval_time = time.time() - start_time
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¥Ù„Ù‰ ØµÙŠØºØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ JSON
            retrieved_docs_serializable = [{"content": doc.page_content, "source": doc.metadata.get("source", "N/A")} for doc in retrieved_docs_langchain]

            rerank_time = 0
            if retriever_type == "hybrid" and cross_encoder:
                print(f"  - ğŸ”ƒ Ø¬Ø§Ø±Ù Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ {len(retrieved_docs_serializable)} Ù…Ø³ØªÙ†Ø¯...")
                rerank_start_time = time.time()
                reranked_docs = rerank_documents(case['question'], retrieved_docs_serializable)
                rerank_time = time.time() - rerank_start_time
                print(f"  - âœ… Ø§ÙƒØªÙ…Ù„Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙÙŠ {rerank_time:.2f} Ø«Ø§Ù†ÙŠØ©.")
                final_docs_to_evaluate = reranked_docs[:5] # Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ 5 Ø¨Ø¹Ø¯ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨
            else:
                final_docs_to_evaluate = retrieved_docs_serializable[:5] # Ù†Ø£Ø®Ø° Ø£ÙØ¶Ù„ 5 Ù…Ø¨Ø§Ø´Ø±Ø©

            evaluation = evaluate_retrieval(
                retrieved_docs=final_docs_to_evaluate,
                expected_keywords=case.get('expected_keywords', []),
                expected_source=case.get('expected_source', '')
            )
            print(f"  - ğŸ“Š Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {evaluation['status']} (Ø§Ù„Ù…ØµØ¯Ø±: {evaluation['source_check']}, Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {evaluation['keyword_evaluation'].get('score', 'N/A')})")

            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ±ØªÙŠØ¨ ÙˆØ§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            for i, doc in enumerate(final_docs_to_evaluate):
                doc['final_rank'] = i + 1

            report["evaluation_results"].append({
                "case_id": case['case_id'],
                "question": case['question'],
                "timing": {
                    "retrieval_seconds": round(retrieval_time, 2),
                    "rerank_seconds": round(rerank_time, 2),
                    "total_seconds": round(retrieval_time + rerank_time, 2)
                },
                "evaluation": evaluation,
                "retrieved_documents": final_docs_to_evaluate
            })

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"{timestamp_str}_{tenant_id}_{retriever_type}.json"
        report_path = os.path.join(RESULTS_DIR, report_filename)
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…ÙØµÙ„ ÙÙŠ: {report_path}")


def main():
    parser = argparse.ArgumentParser(description="Ø¥Ø·Ø§Ø± ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹.")
    parser.add_argument("--tenant", type=str, required=True, help="Ù‡ÙˆÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„ (Ø£Ùˆ 'all' Ù„Ù„Ø¬Ù…ÙŠØ¹).")
    parser.add_argument("--retriever", type=str, required=True, help="Ù†ÙˆØ¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ (faiss, bm25, ensemble, hybrid, Ø£Ùˆ 'all' Ù„Ù„Ø¬Ù…ÙŠØ¹).")
    args = parser.parse_args()

    all_docs = load_all_docs_from_faiss()
    if not all_docs:
        return

    tenants_to_test = [d for d in os.listdir(VECTOR_DB_BASE_DIR) if os.path.isdir(os.path.join(VECTOR_DB_BASE_DIR, d))] if args.tenant == 'all' else [args.tenant]
    retrievers_to_test = ["hybrid", "ensemble", "faiss", "bm25"] if args.retriever == 'all' else [args.retriever]

    for tenant in tenants_to_test:
        run_test_for_tenant(tenant, retrievers_to_test, all_docs)
    
    print("\n" + "="*70)
    print("ğŸ‰ğŸ‰ğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­! ğŸ‰ğŸ‰ğŸ‰")
    print(f"ğŸ” ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø© ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯: {RESULTS_DIR}")
    print("="*70)


if __name__ == "__main__":
    main()
