import os
import shutil
from dotenv import load_dotenv
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader, Docx2txtLoader

# --- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
load_dotenv()
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
if not EMBEDDING_MODEL or not OLLAMA_HOST:
    raise ValueError("EMBEDDING_MODEL_NAME Ùˆ OLLAMA_HOST ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø¹Ø±ÙØ© ÙÙŠ Ù…Ù„Ù .env")

SOURCE_DIR = "4_client_docs"
TARGET_DIR = "3_shared_resources/vector_dbs"

def get_loader(file_path: str):
    ext = file_path.lower().split('.')[-1]
    if ext == 'pdf': return PyPDFLoader(file_path)
    if ext == 'docx': return Docx2txtLoader(file_path)
    return TextLoader(file_path, encoding='utf-8', autodetect_encoding=True)

def build_stores():
    print("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©...")
    
    if os.path.exists(TARGET_DIR):
        print(f"ğŸ§¹ Ù…Ø³Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…: {TARGET_DIR}")
        shutil.rmtree(TARGET_DIR)
    os.makedirs(TARGET_DIR)

    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=250)

    tenants = [d for d in os.listdir(SOURCE_DIR) if os.path.isdir(os.path.join(SOURCE_DIR, d))]
    if not tenants:
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø¬Ù„Ø¯Ø§Øª Ø¹Ù…Ù„Ø§Ø¡ ÙÙŠ '4_client_docs'.")
        return

    for tenant in tenants:
        print(f"\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„: {tenant}...")
        tenant_source_path = os.path.join(SOURCE_DIR, tenant)
        tenant_target_path = os.path.join(TARGET_DIR, tenant)

        try:
            loader = DirectoryLoader(
                tenant_source_path,
                glob="**/*.*",
                loader_cls=get_loader,
                show_progress=True,
                use_multithreading=True,
                silent_errors=True
            )
            docs = loader.load()
            if not docs:
                print(f"ğŸŸ¡ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ù„Ù„Ø¹Ù…ÙŠÙ„: {tenant}")
                continue

            splits = text_splitter.split_documents(docs)
            print(f"ğŸ“„ ØªÙ… ØªÙ‚Ø³ÙŠÙ… Ù…Ø³ØªÙ†Ø¯Ø§Øª {tenant} Ø¥Ù„Ù‰ {len(splits)} Ø¬Ø²Ø¡.")

            vector_store = FAISS.from_documents(splits, embeddings)
            vector_store.save_local(tenant_target_path)
            print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…ÙŠÙ„ '{tenant}' Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ: {tenant_target_path}")

        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„ {tenant}. Ø§Ù„Ø®Ø·Ø£: {e}")

    print("\nğŸ‰ Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø¨Ù†Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")

if __name__ == "__main__":
    build_stores()
