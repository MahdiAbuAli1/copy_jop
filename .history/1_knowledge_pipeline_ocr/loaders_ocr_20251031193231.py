# 1_knowledge_pipeline_ocr/loaders_ocr.py

import os
import json
from typing import List, Tuple, Optional
import fitz  # PyMuPDF
from PIL import Image
import pytesseract
import io
import numpy as np
import cv2  # OpenCV for image pre-processing

from langchain_core.documents import Document
from langchain_community.document_loaders import (
    UnstructuredWordDocumentLoader,
    TextLoader,
)

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª OCR ---
TESSERACT_LANG = 'ara+eng'
# PSM 3: ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„ØªØ®Ø·ÙŠØ· Ø§Ù„ØµÙØ­Ø©ØŒ ÙˆÙ‡Ùˆ Ø®ÙŠØ§Ø± Ù‚ÙˆÙŠ ÙˆÙ…ØªÙˆØ§Ø²Ù†.
# DPI 300: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ù‚Ø© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ø±Ù.
TESSERACT_CONFIG = '--psm 3 --dpi 300'

def is_text_meaningful(text: str, min_chars: int = 15, min_words: int = 3) -> bool:
    """
    Ø¯Ø§Ù„Ø© Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø°Ø§ Ù…Ø¹Ù†Ù‰ Ø£Ù… Ù…Ø¬Ø±Ø¯ Ø¶ÙˆØ¶Ø§Ø¡.
    ØªÙ… Ø¬Ø¹Ù„Ù‡Ø§ Ø£ÙƒØ«Ø± ØµØ±Ø§Ù…Ø©.
    """
    text = text.strip()
    if len(text) < min_chars:
        return False
    words = text.split()
    if len(words) < min_words:
        return False
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ
    alpha_chars = sum(1 for char in text if char.isalpha())
    if alpha_chars / len(text) < 0.6:  # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† 60% Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù†Øµ Ø­Ø±ÙˆÙÙ‹Ø§
        return False
    return True

def deskew_image(image: np.ndarray) -> np.ndarray:
    """
    ØªØµØ­ÙŠØ­ Ù…ÙŠÙ„Ø§Ù† Ø§Ù„ØµÙˆØ±Ø© Ù„Ø¬Ø¹Ù„ Ø§Ù„Ù†Øµ Ø£ÙÙ‚ÙŠÙ‹Ø§ ØªÙ…Ø§Ù…Ù‹Ø§.
    """
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø£Ø¨ÙŠØ¶ ÙˆØ£Ø³ÙˆØ¯ ÙˆØ§Ù„Ø¹ÙƒØ³ Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø·ÙˆØ·
        gray = cv2.bitwise_not(image)
        coords = np.column_stack(np.where(gray > 0))
        angle = cv2.minAreaRect(coords)[-1]

        if angle < -45:
            angle = -(90 + angle)
        else:
            angle = -angle
        
        if abs(angle) > 20: # Ù„Ø§ ØªÙ‚Ù… Ø¨Ø§Ù„ØªØµØ­ÙŠØ­ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø²Ø§ÙˆÙŠØ© ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ù‹Ø§ (Ù‚Ø¯ ØªÙƒÙˆÙ† ØµÙˆØ±Ø© Ù…Ø§Ø¦Ù„Ø© Ø¹Ù…Ø¯Ù‹Ø§)
            return image

        (h, w) = image.shape[:2]
        center = (w // 2, h // 2)
        M = cv2.getRotationMatrix2D(center, angle, 1.0)
        rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
        return rotated
    except Exception:
        return image # ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ø¯ÙˆØ« Ø£ÙŠ Ø®Ø·Ø£ØŒ Ø£Ø±Ø¬Ø¹ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©

def preprocess_image_for_ocr(image_bytes: bytes) -> Image.Image:
    """
    Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© ØµÙˆØ± Ù…ØªÙ‚Ø¯Ù…Ø© Ø¬Ø¯Ù‹Ø§ Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‚ØµÙ‰ Ø¯Ù‚Ø© Ù…Ù† Tesseract.
    """
    nparr = np.frombuffer(image_bytes, np.uint8)
    img_cv = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
    
    # 1. ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ¯Ø±Ø¬ Ø§Ù„Ø±Ù…Ø§Ø¯ÙŠ
    gray_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)
    
    # 2. ØªØµØ­ÙŠØ­ Ø§Ù„Ù…ÙŠÙ„Ø§Ù† (Deskewing)
    deskewed_img = deskew_image(gray_img)
    
    # 3. ØªÙƒØ¨ÙŠØ± Ø§Ù„ØµÙˆØ±Ø© (Upscaling) Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙØ§ØµÙŠÙ„
    scale_factor = 2.0
    width = int(deskewed_img.shape[1] * scale_factor)
    height = int(deskewed_img.shape[0] * scale_factor)
    resized_img = cv2.resize(deskewed_img, (width, height), interpolation=cv2.INTER_LANCZOS4)
    
    # 4. ØªØ·Ø¨ÙŠÙ‚ ÙÙ„ØªØ± Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙˆÙŠØ´
    denoised_img = cv2.fastNlMeansDenoising(resized_img, h=30, templateWindowSize=7, searchWindowSize=21)
    
    # 5. ØªØ·Ø¨ÙŠÙ‚ Adaptive Thresholding Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø­Ø±ÙˆÙ Ø¨Ø§Ø±Ø²Ø©
    processed_img = cv2.adaptiveThreshold(
        denoised_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
        cv2.THRESH_BINARY, 11, 5
    )
    
    return Image.fromarray(processed_img)

# --- Ù…Ø­Ù…Ù„ PDF Ø§Ù„Ù…Ø­Ù„ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø¯Ù‚Ø© ---
class HighAccuracyLocalPdfLoader:
    def __init__(self, file_path: str):
        self.file_path = file_path
        print(f"ğŸš€ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ù…Ù„ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø¯Ù‚Ø© Ù„Ù„Ù…Ù„Ù: {os.path.basename(self.file_path)}")

    def load(self) -> List[Document]:
        docs = []
        try:
            pdf_document = fitz.open(self.file_path)
            print(f"    - ğŸ“– Ø¬Ø§Ø±Ù Ù…Ø¹Ø§Ù„Ø¬Ø© {len(pdf_document)} ØµÙØ­Ø©...")
            
            for page_num, page in enumerate(pdf_document):
                normal_text = page.get_text("text").strip()
                ocr_texts = []
                image_list = page.get_images(full=True)
                
                if image_list:
                    print(f"      - ğŸ–¼ï¸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_list)} ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num + 1}. Ø¬Ø§Ø±Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ø¯Ù‚ÙŠÙ‚...")
                    for img_index, img in enumerate(image_list):
                        xref = img[0]
                        base_image = pdf_document.extract_image(xref)
                        image_bytes = base_image["image"]
                        
                        try:
                            # 1. Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ø¬Ø¯Ù‹Ø§ Ù„Ù„ØµÙˆØ±Ø©
                            preprocessed_image = preprocess_image_for_ocr(image_bytes)
                            
                            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Tesseract
                            ocr_text = pytesseract.image_to_string(
                                preprocessed_image, 
                                lang=TESSERACT_LANG,
                                config=TESSERACT_CONFIG
                            )
                            
                            # 3. ÙÙ„ØªØ±Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¶Ø¹ÙŠÙØ©
                            if is_text_meaningful(ocr_text):
                                print(f"        - âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© {img_index + 1}.")
                                ocr_texts.append(ocr_text.strip())
                            else:
                                print(f"        - ğŸ—‘ï¸ ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ù†Øµ ØºÙŠØ± Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙˆØ±Ø© {img_index + 1}.")

                        except Exception as ocr_e:
                            print(f"        - âš ï¸ ÙØ´Ù„ ØªØ­Ù„ÙŠÙ„ ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„ØµÙØ­Ø© {page_num + 1}. Ø§Ù„Ø®Ø·Ø£: {ocr_e}")

                page_content_parts = []
                if normal_text:
                    page_content_parts.append("--- Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ ---\n" + normal_text)
                if ocr_texts:
                    full_ocr_text = "\n\n".join(ocr_texts)
                    page_content_parts.append("--- Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ØµÙˆØ± (OCR) ---\n" + full_ocr_text)
                
                final_page_content = "\n\n".join(page_content_parts)
                
                if final_page_content:
                    metadata = {"source": self.file_path, "page": page_num + 1}
                    docs.append(Document(page_content=final_page_content, metadata=metadata))
                
            pdf_document.close()
        except Exception as e:
            print(f"    - âŒ ÙØ´Ù„ ÙƒØ¨ÙŠØ± ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF '{self.file_path}'. Ø§Ù„Ø®Ø·Ø£: {e}")
            return []
            
        return docs

# --- ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªØ­Ù…ÙŠÙ„ ---
LOADER_MAPPING = {
    ".pdf": HighAccuracyLocalPdfLoader,  # <-- âœ¨âœ¨ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙØ§Ø¦Ù‚ Ø§Ù„Ø¯Ù‚Ø© Ù‡Ù†Ø§ âœ¨âœ¨
    ".docx": UnstructuredWordDocumentLoader,
    ".txt": TextLoader,
}

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (ØªØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡ÙŠ) ---
def load_documents(source_dir: str) -> Tuple[List[Document], Optional[str]]:
    # ... (Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§ Ù„Ù… ÙŠØªØºÙŠØ±) ...
    all_documents = []
    entity_name = None
    config_file_path = os.path.join(source_dir, "config.json")

    print(f"ğŸ“‚ Ø¬Ø§Ø±Ù Ø§Ù„Ù…Ø³Ø­ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: '{source_dir}'")

    if not os.path.isdir(source_dir):
        raise ValueError(f"Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ Ù„ÙŠØ³ Ù…Ø¬Ù„Ø¯Ù‹Ø§ ØµØ§Ù„Ø­Ù‹Ø§: {source_dir}")

    if os.path.exists(config_file_path):
        try:
            with open(config_file_path, "r", encoding="utf-8") as f:
                config_data = json.load(f)
                entity_name = config_data.get("entity_name")
                if entity_name:
                    print(f"  - âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ø³Ù… Ø§Ù„ÙƒÙŠØ§Ù†: '{entity_name}'")
        except Exception as e:
            print(f"  - âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù‚Ø±Ø§Ø¡Ø© 'config.json': {e}")
    else:
        print(f"  - âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù 'config.json'.")

    for filename in os.listdir(source_dir):
        if filename == "config.json" or filename.startswith('.'):
            continue
        
        file_path = os.path.join(source_dir, filename)
        if not os.path.isfile(file_path):
            continue

        file_ext = os.path.splitext(filename)[1].lower()
        if file_ext in LOADER_MAPPING:
            loader_class = LOADER_MAPPING[file_ext]
            print(f"  - ğŸ“„ Ø¬Ø§Ø±Ù ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù: '{filename}'...")
            try:
                loader = loader_class(file_path)
                loaded_docs = loader.load()
                all_documents.extend(loaded_docs)
                print(f"    - âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© {len(loaded_docs)} ØµÙØ­Ø©.")
            except Exception as e:
                print(f"    - âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù '{filename}'. Ø§Ù„Ø®Ø·Ø£: {e}")
        else:
            print(f"  -  ØªÙ… ØªØ®Ø·ÙŠ Ù…Ù„Ù ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: '{filename}'")

    if not all_documents:
        print(" Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
    
    print(f"\n Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„. Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {len(all_documents)}")
    return all_documents, entity_name
