# # src/app/core_logic.py
# #ÙƒÙˆØ¯ Ù…Ù…ØªØ§Ø² Ø§Ø«Ø¨Øª Ø¬Ø¯Ø§Ø±ØªÙ‡ ÙˆÙ†ØªØ§Ø¦Ø¬ Ù…Ù…ØªØ§Ø²Ù‡
# import os
# import logging
# import asyncio
# import httpx
# from typing import AsyncGenerator, Dict, List

# from dotenv import load_dotenv
# from langchain_community.embeddings import OllamaEmbeddings
# from langchain_community.vectorstores import FAISS
# from langchain_community.llms import Ollama
# from langchain_core.prompts import ChatPromptTemplate
# from langchain.chains import create_history_aware_retriever, create_retrieval_chain
# from langchain.chains.combine_documents import create_stuff_documents_chain
# from langchain_core.messages import HumanMessage, AIMessage

# # --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
# PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__ ), "../../"))
# load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# # --- Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø³ÙƒØ±Øª Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ ---
# EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:4b")
# CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
# OLLAMA_HOST = os.getenv("OLLAMA_HOST")

# # --- Ø§Ù„Ù…Ø³Ø§Ø± Ø¥Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© Ø§Ù„ØªÙŠ ÙŠØ¨Ù†ÙŠÙ‡Ø§ Ø³ÙƒØ±Øª main_builder.py ---
# UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# # --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
# llm: Ollama = None
# vector_store: FAISS = None
# embeddings: OllamaEmbeddings = None
# chat_history: Dict[str, List[HumanMessage | AIMessage]] = {} 
# initialization_lock = asyncio.Lock()

# # --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ ---
# REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
# Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
# Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
# Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

# ANSWER_PROMPT = ChatPromptTemplate.from_template("""
# Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
# - ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
# - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
# - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
# - Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

# Ø§Ù„Ø³ÙŠØ§Ù‚:
# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
# Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# # --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---
# async def initialize_agent():
#     global llm, embeddings, vector_store
#     async with initialization_lock:
#         if vector_store is not None: return
#         logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø©...")
#         try:
#             async with httpx.AsyncClient( ) as client:
#                 await client.get(OLLAMA_HOST, timeout=10.0)
#             llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
#             embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            
#             if not os.path.isdir(UNIFIED_DB_PATH):
#                 raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Øª 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")

#             vector_store = await asyncio.to_thread(
#                 FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
#             )
#             logging.info("âœ… Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ­Ø¯Ø©.")
#         except Exception as e:
#             logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
#             raise

# # --- 4. Ø¯Ø§Ù„Ø© get_answer_stream ---
# async def get_answer_stream(request_info: dict) -> AsyncGenerator[Dict, None]:
#     question = request_info["question"].strip()
#     tenant_id = request_info.get("tenant_id")
#     session_id = tenant_id or "default_session"

#     if not vector_store:
#         yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
#         return

#     # --- Ø§Ù„ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…ÙŠÙ„ ØªØªÙ… Ù‡Ù†Ø§ØŒ ÙÙŠ Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø¨Ø­Ø« ---
#     retriever = vector_store.as_retriever(
#         search_kwargs={'k': 15, 'filter': {'tenant_id': tenant_id}}
#     )
    
#     user_chat_history = chat_history.get(session_id, [])

#     history_aware_retriever = create_history_aware_retriever(llm, retriever, REPHRASE_PROMPT)
#     document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
#     conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

#     logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
#     try:
#         full_answer = ""
#         async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
#             if "answer" in chunk and chunk["answer"] is not None:
#                 answer_chunk = chunk["answer"]
#                 full_answer += answer_chunk
#                 yield {"type": "chunk", "content": answer_chunk}
        
#         user_chat_history.append(HumanMessage(content=question))
#         user_chat_history.append(AIMessage(content=full_answer))
#         chat_history[session_id] = user_chat_history[-10:]
#         logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
#     except Exception as e:
#         logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
#         yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}

import os
import logging
import asyncio
import httpx
from typing import AsyncGenerator, Dict, List, cast

from dotenv import load_dotenv
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document

# --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---
# --- Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# --- vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv ---

# 1. Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†
from langchain.retrievers import BM25Retriever, EnsembleRetriever

# 2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø³Ù„Ø§Ø³Ù„
# Ù‡Ø°Ø§ ÙŠØ­Ù„ Ù…Ø´ÙƒÙ„Ø© ImportError
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain

# --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---
# --- Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ù‚Ø³Ù… Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ¹Ø¯ÙŠÙ„Ù‡ ---
# --- ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ ---

from .performance_tracker import PerformanceLogger

# --- 1. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§ ) ---
# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¨Ù‚Ù‰ ÙƒÙ…Ø§ Ù‡Ùˆ Ø¯ÙˆÙ† Ø£ÙŠ ØªØºÙŠÙŠØ±) ...
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))
load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, ".env"))
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL_NAME", "qwen3-embedding:0.6b")
CHAT_MODEL = os.getenv("CHAT_MODEL_NAME", "qwen2:7b-instruct-q3_K_M")
OLLAMA_HOST = os.getenv("OLLAMA_HOST")
UNIFIED_DB_PATH = os.path.join(PROJECT_ROOT, "3_shared_resources", "vector_db")

# --- Ù…ØªØºÙŠØ±Ø§Øª Ø¹Ø§Ù„Ù…ÙŠØ© ---
llm: Ollama = None
ensemble_retriever: EnsembleRetriever = None 
chat_history: Dict[str, List[HumanMessage | AIMessage]] = {}
initialization_lock = asyncio.Lock()
perf_logger = PerformanceLogger()

# --- 2. Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
REPHRASE_PROMPT = ChatPromptTemplate.from_template("""
Ø¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙˆØ§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ù‚Ù… Ø¨ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„ Ù…Ø³ØªÙ‚Ù„ ÙŠÙ…ÙƒÙ† ÙÙ‡Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©.
Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {chat_history}
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø£Ø®ÙŠØ±: {input}
Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªÙ‚Ù„:""")

ANSWER_PROMPT = ChatPromptTemplate.from_template("""
Ø£Ù†Øª "Ù…Ø±Ø´Ø¯ Ø§Ù„Ø¯Ø¹Ù…"ØŒ Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ®Ø¨ÙŠØ±. Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ **Ø­ØµØ±ÙŠØ§Ù‹** Ø¹Ù„Ù‰ "Ø§Ù„Ø³ÙŠØ§Ù‚" Ø§Ù„Ù…Ù‚Ø¯Ù….
- ÙƒÙ† Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…ØªØ¹Ø§ÙˆÙ†Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹.
- Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø©ØŒ Ù‚Ø¯Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆÙ…Ù†Ø¸Ù….
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„ Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù„Ø·ÙŠÙ: "Ø¨Ø­Ø«Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£Ø¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© Ø¨Ø®ØµÙˆØµ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„."
- Ù„Ø§ ØªØ®ØªØ±Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø¨Ø¯Ø§Ù‹. Ø§Ù„ØªØ²Ù… Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚.

Ø§Ù„Ø³ÙŠØ§Ù‚:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {input}
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:""")

# --- 3. Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
def _load_all_docs_from_faiss(vector_store: FAISS) -> List[Document]:
    return list(cast(dict, vector_store.docstore._dict).values())

async def initialize_agent():
    global llm, ensemble_retriever
    async with initialization_lock:
        if ensemble_retriever is not None: return
        logging.info("Ø¨Ø¯Ø¡ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†...")
        try:
            async with httpx.AsyncClient( ) as client:
                await client.get(OLLAMA_HOST, timeout=10.0)
            llm = Ollama(model=CHAT_MODEL, base_url=OLLAMA_HOST, temperature=0.1)
            
            logging.info("ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
            if not os.path.isdir(UNIFIED_DB_PATH):
                raise FileNotFoundError(f"Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ­Ø¯Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ 'main_builder.py' Ø£ÙˆÙ„Ø§Ù‹.")
            
            embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=OLLAMA_HOST)
            faiss_vector_store = await asyncio.to_thread(
                FAISS.load_local, UNIFIED_DB_PATH, embeddings, allow_dangerous_deserialization=True
            )
            faiss_retriever = faiss_vector_store.as_retriever(search_kwargs={'k': 4})
            logging.info("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (FAISS).")

            logging.info("Ø¨Ù†Ø§Ø¡ Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© (BM25)...")
            all_docs = await asyncio.to_thread(_load_all_docs_from_faiss, faiss_vector_store)
            bm25_retriever = BM25Retriever.from_documents(all_docs)
            bm25_retriever.k = 4
            logging.info("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ (BM25).")

            ensemble_retriever = EnsembleRetriever(
                retrievers=[bm25_retriever, faiss_retriever],
                weights=[0.5, 0.5]
            )
            logging.info("ğŸš€ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ†.")

        except Exception as e:
            logging.error(f"ÙØ´Ù„ ÙØ§Ø¯Ø­ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©: {e}", exc_info=True)
            raise

# --- 4. Ø¯Ø§Ù„Ø© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¬Ø§Ù‡Ø²ÙŠØ© Ø§Ù„ÙˆÙƒÙŠÙ„ (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
def agent_ready() -> bool:
    return ensemble_retriever is not None and llm is not None

# --- 5. Ø¯Ø§Ù„Ø© get_answer_stream (Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§) ---
async def get_answer_stream(request_info: Dict) -> AsyncGenerator[Dict, None]:
    question = request_info.get("question", "")
    tenant_id = request_info.get("tenant_id", "default_session")
    
    session_id = tenant_id or "default_session"

    if not ensemble_retriever:
        yield {"type": "error", "content": "Ø§Ù„ÙˆÙƒÙŠÙ„ ØºÙŠØ± Ø¬Ø§Ù‡Ø². ÙŠØ±Ø¬Ù‰ Ø¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©."}
        return

    perf_logger.start("total_request", tenant_id, question, {"retriever_type": "hybrid"})
    
    user_chat_history = chat_history.get(session_id, [])

    history_aware_retriever = create_history_aware_retriever(llm, ensemble_retriever, REPHRASE_PROMPT)
    document_chain = create_stuff_documents_chain(llm, ANSWER_PROMPT)
    conversational_rag_chain = create_retrieval_chain(history_aware_retriever, document_chain)

    logging.info(f"[{session_id}] Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ '{question}'...")
    try:
        full_answer = ""
        perf_logger.start("llm_stream_generation", tenant_id, question)

        async for chunk in conversational_rag_chain.astream({"input": question, "chat_history": user_chat_history}):
            if "answer" in chunk and chunk["answer"] is not None:
                answer_chunk = chunk["answer"]
                full_answer += answer_chunk
                yield {"type": "chunk", "content": answer_chunk}
        
        perf_logger.end("llm_stream_generation", tenant_id, question, {"answer_length": len(full_answer)})

        user_chat_history.append(HumanMessage(content=question))
        user_chat_history.append(AIMessage(content=full_answer))
        chat_history[session_id] = user_chat_history[-10:]
        logging.info(f"[{session_id}] Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©: '{full_answer}'")
    except Exception as e:
        logging.error(f"[{session_id}] ÙØ´Ù„ ÙÙŠ Ø³Ù„Ø³Ù„Ø© RAG. Ø§Ù„Ø®Ø·Ø£: {e}", exc_info=True)
        yield {"type": "error", "content": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙØ§Ø¯Ø­."}
    finally:
        perf_logger.end("total_request", tenant_id, question)
